{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9LLxe7K75tG"
      },
      "source": [
        "## 1.Environment Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuFAtJbe74-k",
        "outputId": "570b0ea8-00c2-44bf-a67a-96cea84d314b"
      },
      "outputs": [],
      "source": [
        "# === 1.3 Import Libraries ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "from surprise import SVDpp\n",
        "from surprise import Dataset, Reader \n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "from surprise.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define path\n",
        "data_path = '../datasets/student_grade.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "v3I6DeUo3B7a",
        "outputId": "c1de57c2-a487-4965-cec5-7f691907f637"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_path, low_memory=False)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s7YDtxwjreA"
      },
      "source": [
        "## 2.Data Loading & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "collapsed": true,
        "id": "Vcf-pwd3_NtF",
        "outputId": "807d340e-c729-43a9-eb1f-0804c4e16343"
      },
      "outputs": [],
      "source": [
        "# === 2.1 Load Data ===\n",
        "df = pd.read_csv(data_path, low_memory=False)\n",
        "\n",
        "# === 2.2 Transform Data (Wide to Long) ===\n",
        "id_vars = ['student_id']\n",
        "df_long = pd.melt(df, id_vars=id_vars, var_name='course', value_name='grade')\n",
        "\n",
        "# === 2.3 Clean Data ===\n",
        "# Convert grade to numeric and remove invalid/empty grades\n",
        "df_long['grade'] = pd.to_numeric(df_long['grade'], errors='coerce')\n",
        "df_long_cleaned = df_long[(df_long['grade'] > 0.0) & (df_long['grade'].notna())].copy()\n",
        "\n",
        "# === 2.4 Filter for 'INT' Courses Only ===\n",
        "# This ensures the model only learns from INT courses\n",
        "df_long_filtered = df_long_cleaned[df_long_cleaned['course'].astype(str).str.startswith('INT')].copy()\n",
        "\n",
        "print(f\"--- Data Preparation Complete ---\")\n",
        "print(f\"Total records after cleaning: {len(df_long_cleaned)}\")\n",
        "print(f\"Filtered to INT courses only: {len(df_long_filtered)}\")\n",
        "display(df_long_filtered)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2PvDHolfQQl"
      },
      "source": [
        "## 3.Split Data to train and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PW9IQUP-fhEA"
      },
      "outputs": [],
      "source": [
        "# === 3.1 Load Data into Surprise Dataset ===\n",
        "# Define rating scale (assuming grades are 1.0 to 4.0)\n",
        "reader = Reader(rating_scale=(1, 4))\n",
        "data = Dataset.load_from_df(df_long_filtered[['student_id', 'course', 'grade']], reader)\n",
        "\n",
        "\n",
        "# === 3.2 Split Data ===\n",
        "trainset, testset = train_test_split(data, test_size=0.30, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u15hmTyy9i-J"
      },
      "source": [
        "## 4.Model Training (SVD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HTGZpnG9m-1"
      },
      "outputs": [],
      "source": [
        "# === 3.3 Train the Model ===\n",
        "print(\"--- Training SVD Model ---\")\n",
        "model = SVDpp(\n",
        "    n_factors = 150,\n",
        "    n_epochs  = 250,\n",
        "    lr_all    = 0.01,\n",
        "    reg_all   = 0.05,\n",
        "    random_state = 42\n",
        ")\n",
        "model.fit(trainset)\n",
        "print(\"Training complete.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcVzmwqJgwE8"
      },
      "outputs": [],
      "source": [
        "model.predict('A458','INT540 SELECTED TOPIC IN INFORMATION TECHNOLOGY : BUSINESS FINANCE AND DATA ANALYTICS')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPJ-s7_-gIR6"
      },
      "source": [
        "## 5.Test and evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tvham1r6gNnj"
      },
      "outputs": [],
      "source": [
        "# === 3.4 Evaluate Performance ===\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "predictions = model.test(testset)\n",
        "rmse = accuracy.rmse(predictions)\n",
        "mae = accuracy.mae(predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snjCtej696cC"
      },
      "outputs": [],
      "source": [
        "# === 4.1 Predict for Unknown Items ===\n",
        "print(\"--- Generating Predictions for all missing pairs ---\")\n",
        "anti_testset = trainset.build_anti_testset()\n",
        "all_predictions = model.test(anti_testset)\n",
        "\n",
        "# === 4.2 Helper Function for Top-N ===\n",
        "def get_top_n(predictions, n=5):\n",
        "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\"\"\"\n",
        "    top_n = defaultdict(list)\n",
        "    for uid, iid, true_r, est, _ in predictions:\n",
        "        top_n[uid].append((iid, est))\n",
        "\n",
        "    for uid, user_ratings in top_n.items():\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_n[uid] = user_ratings[:n]\n",
        "\n",
        "    return top_n\n",
        "\n",
        "# === 4.3 Generate Top 5 Recommendations ===\n",
        "top_n_recommendations = get_top_n(all_predictions, n=5)\n",
        "print(f\"Generated recommendations for {len(top_n_recommendations)} students.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHpNvrN6-ESD"
      },
      "outputs": [],
      "source": [
        "# === 5.1 Visualization Function ===\n",
        "def visualize_topk_by_rank(top_n_recommendations, K=5, top_m=10):\n",
        "    \"\"\"\n",
        "    Visualizes the frequency of recommended courses by rank.\n",
        "    K: Number of recommendations per student.\n",
        "    top_m: Number of top courses to show in the chart.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for student_id, recs in top_n_recommendations.items():\n",
        "        for r, (course, score) in enumerate(recs[:K], start=1):\n",
        "            rows.append({\"rank\": r, \"course\": course})\n",
        "\n",
        "    if not rows:\n",
        "        print(\"No recommendations to visualize.\")\n",
        "        return\n",
        "\n",
        "    df_viz = pd.DataFrame(rows)\n",
        "    rank_counters = {r: Counter(df_viz[df_viz[\"rank\"] == r][\"course\"]) for r in range(1, K+1)}\n",
        "\n",
        "    for r in range(1, K+1):\n",
        "        counter = rank_counters[r]\n",
        "        if not counter: continue\n",
        "\n",
        "        most_common = counter.most_common(top_m)\n",
        "        courses = [c for c, _ in most_common]\n",
        "        counts  = [cnt for _, cnt in most_common]\n",
        "\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.bar(courses, counts, color='skyblue')\n",
        "        plt.title(f\"Top {top_m} Courses @ Rank {r}\")\n",
        "        plt.xlabel(\"Course\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.xticks(rotation=45, ha=\"right\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# === 5.2 Run Visualization ===\n",
        "print(\"--- Visualization of Recommendations ---\")\n",
        "visualize_topk_by_rank(top_n_recommendations, K=5, top_m=10)\n",
        "\n",
        "# === 5.3 Create Final DataFrame ===\n",
        "rows = []\n",
        "for student_id, recs in top_n_recommendations.items():\n",
        "    for rank, (course, predicted_grade) in enumerate(recs, start=1):\n",
        "        # Note: Data was filtered for INT in step 2, so this check is just a safeguard\n",
        "        if isinstance(course, str) and course.startswith(\"INT\"):\n",
        "            rows.append({\n",
        "                \"student_id\": student_id,\n",
        "                \"rank\": rank,\n",
        "                \"course\": course,\n",
        "                \"predicted_grade\": predicted_grade\n",
        "            })\n",
        "\n",
        "df_recommendations = pd.DataFrame(rows)\n",
        "\n",
        "print(\"\\n--- Final Recommendations Preview ---\")\n",
        "display(df_recommendations.head(15))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ-UK6fV9mgy"
      },
      "source": [
        "## 6.Confusion Matrix 8 grades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yjtQ4yN9pLN"
      },
      "outputs": [],
      "source": [
        "# 1. Define Mapping: Score -> Letter\n",
        "# ต้องเรียงลำดับจากน้อยไปมาก เพื่อให้ Matrix สวยงาม\n",
        "score_to_letter = {\n",
        "    0.0: 'F', 1.0: 'D', 1.5: 'D+', 2.0: 'C', 2.5: 'C+',\n",
        "    3.0: 'B', 3.5: 'B+', 4.0: 'A'\n",
        "}\n",
        "\n",
        "# สร้าง list ของเกรดที่เป็นไปได้ (เอาไว้ระบุ Labels)\n",
        "# กรองเอาเฉพาะที่มีใน map (เผื่อกรณีข้อมูลไม่มี F)\n",
        "valid_scores = sorted(score_to_letter.keys())\n",
        "valid_labels = [score_to_letter[s] for s in valid_scores]\n",
        "\n",
        "# 2. Helper function: หาเกรดที่ใกล้ที่สุด\n",
        "def get_nearest_grade_key(pred_score):\n",
        "    # หา key (คะแนน) ที่ใกล้ที่สุด\n",
        "    return min(valid_scores, key=lambda x: abs(x - pred_score))\n",
        "\n",
        "# 3. Prepare Data\n",
        "y_true_letters = []\n",
        "y_pred_letters = []\n",
        "\n",
        "for pred in predictions:\n",
        "    # 3.1 แปลง Actual Grade (r_ui) เป็น Letter\n",
        "    # ใช้ get_nearest_grade_key เผื่อค่า r_ui มีทศนิยมเพี้ยนเล็กน้อย\n",
        "    true_score_key = get_nearest_grade_key(pred.r_ui)\n",
        "    y_true_letters.append(score_to_letter[true_score_key])\n",
        "\n",
        "    # 3.2 แปลง Predicted Grade (est) เป็น Letter\n",
        "    pred_score_key = get_nearest_grade_key(pred.est)\n",
        "    y_pred_letters.append(score_to_letter[pred_score_key])\n",
        "\n",
        "# 4. Generate Confusion Matrix\n",
        "# สำคัญ: ต้องใส่ labels=valid_labels เพื่อบังคับลำดับ (D -> A)\n",
        "cm = confusion_matrix(y_true_letters, y_pred_letters, labels=valid_labels)\n",
        "\n",
        "# 5. Plot Heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=valid_labels,\n",
        "            yticklabels=valid_labels)\n",
        "\n",
        "plt.title('Confusion Matrix: Actual vs Predicted Grades')\n",
        "plt.xlabel('Predicted Grade')\n",
        "plt.ylabel('Actual Grade')\n",
        "plt.show()\n",
        "\n",
        "# 6. Classification Report\n",
        "print(\"\\n--- Detailed Classification Report ---\")\n",
        "print(classification_report(y_true_letters, y_pred_letters, target_names=valid_labels, zero_division=0,labels=valid_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.1 Confusion Matrix 5 grades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mapping: 5 Grades (Collapsing Plus grades)\n",
        "score_to_letter_5 = {\n",
        "    0.0: 'F', \n",
        "    1.0: 'D', 1.5: 'D', \n",
        "    2.0: 'C', 2.5: 'C',\n",
        "    3.0: 'B', 3.5: 'B', \n",
        "    4.0: 'A'\n",
        "}\n",
        "valid_labels_5 = ['F', 'D', 'C', 'B', 'A']\n",
        "# We still check against the full range of possible scores\n",
        "valid_scores_all = [0.0, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]\n",
        "\n",
        "y_true_5 = [score_to_letter_5[get_nearest_grade_key(p.r_ui)] for p in predictions]\n",
        "y_pred_5 = [score_to_letter_5[get_nearest_grade_key(p.est)] for p in predictions]\n",
        "\n",
        "# Plot Heatmap 5 Grades\n",
        "plt.figure(figsize=(10, 8))\n",
        "cm_5 = confusion_matrix(y_true_5, y_pred_5, labels=valid_labels_5)\n",
        "sns.heatmap(cm_5, annot=True, fmt='d', cmap='Greens', xticklabels=valid_labels_5, yticklabels=valid_labels_5)\n",
        "plt.title('Confusion Matrix: 5 Grades')\n",
        "plt.xlabel('Predicted Grade')\n",
        "plt.ylabel('Actual Grade')\n",
        "\n",
        "print(\"\\n--- 5-Grade Classification Report ---\")\n",
        "print(classification_report(y_true_5, y_pred_5, target_names=valid_labels_5, labels=valid_labels_5, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NK-V5Q__-GT"
      },
      "outputs": [],
      "source": [
        "y_true_raw = [pred.r_ui for pred in predictions]\n",
        "y_pred_raw = [pred.est for pred in predictions]\n",
        "\n",
        "y_pred_rounded = [np.round(pred.est * 2) / 2 for pred in predictions]\n",
        "\n",
        "# 2. คำนวณ R2 Score\n",
        "r2 = r2_score(y_true_raw, y_pred_raw)\n",
        "r2Rounded = r2_score(y_true_raw, y_pred_rounded)\n",
        "\n",
        "\n",
        "print(f\"R2 Score: {r2:.4f}\")\n",
        "print(f\"R2 Score Rounded: {r2Rounded:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ttAB6hiC662",
        "outputId": "1ed6e7b7-c16a-4302-9120-f0b44e1da619"
      },
      "outputs": [],
      "source": [
        "for epoch in range(5):\n",
        "    nmf = SVD(n_factors=3, n_epochs=epoch+1, random_state=42)\n",
        "    nmf.fit(trainset)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}\")\n",
        "    print(nmf.pu[:3])   # user 3 คนแรก"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "surprise-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
