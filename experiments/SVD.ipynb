{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9LLxe7K75tG"
      },
      "source": [
        "## 1.Environment Setup & Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuFAtJbe74-k",
        "outputId": "f7fc8c3c-feee-4442-ce28-6496afbe62d3"
      },
      "outputs": [],
      "source": [
        "# === 1.3 Import Libraries ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "from surprise import SVD\n",
        "from surprise import Dataset, Reader\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "from surprise.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define path\n",
        "data_path = '../datasets/student_grade.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_path, low_memory=False)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s7YDtxwjreA"
      },
      "source": [
        "## 2.Data Loading & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "collapsed": true,
        "id": "Vcf-pwd3_NtF",
        "outputId": "91c2a720-0d30-4742-d2de-67f2da0c2e64"
      },
      "outputs": [],
      "source": [
        "# === 2.1 Load Data ===\n",
        "df = pd.read_csv(data_path, low_memory=False)\n",
        "\n",
        "# === 2.2 Transform Data (Wide to Long) ===\n",
        "id_vars = ['student_id']\n",
        "df_long = pd.melt(df, id_vars=id_vars, var_name='course', value_name='grade')\n",
        "\n",
        "# === 2.3 Clean Data ===\n",
        "# Convert grade to numeric and remove invalid/empty grades\n",
        "df_long['grade'] = pd.to_numeric(df_long['grade'], errors='coerce')\n",
        "df_long_cleaned = df_long[(df_long['grade'] > 0.0) & (df_long['grade'].notna())].copy()\n",
        "\n",
        "# === 2.4 Filter for 'INT' Courses Only ===\n",
        "# This ensures the model only learns from INT courses\n",
        "df_long_filtered = df_long_cleaned[df_long_cleaned['course'].astype(str).str.startswith('INT')].copy()\n",
        "\n",
        "print(f\"--- Data Preparation Complete ---\")\n",
        "print(f\"Total records after cleaning: {len(df_long_cleaned)}\")\n",
        "print(f\"Filtered to INT courses only: {len(df_long_filtered)}\")\n",
        "display(df_long_filtered)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2PvDHolfQQl"
      },
      "source": [
        "## 3.Split Data to train and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PW9IQUP-fhEA"
      },
      "outputs": [],
      "source": [
        "# === 3.1 Load Data into Surprise Dataset ===\n",
        "# Define rating scale (assuming grades are 1.0 to 4.0)\n",
        "reader = Reader(rating_scale=(1, 4))\n",
        "data = Dataset.load_from_df(df_long_filtered[['student_id', 'course', 'grade']], reader)\n",
        "\n",
        "# === 3.2 Split Data ===\n",
        "trainset, testset = train_test_split(data, test_size=0.30, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.Model Training (SVD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HTGZpnG9m-1",
        "outputId": "e321fbc7-5308-4f53-9e37-4e2ee50fa024"
      },
      "outputs": [],
      "source": [
        "# === 4.1 Train the Model ===\n",
        "print(\"--- Training SVD Model ---\")\n",
        "model = SVD(\n",
        "    n_factors = 100,\n",
        "    n_epochs  = 140,\n",
        "    lr_all    = 0.01,\n",
        "    reg_all   = 0.05,\n",
        "    random_state = 42\n",
        ")\n",
        "model.fit(trainset)\n",
        "print(\"Training complete.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcVzmwqJgwE8"
      },
      "outputs": [],
      "source": [
        "model.predict('A458','INT540 SELECTED TOPIC IN INFORMATION TECHNOLOGY : BUSINESS FINANCE AND DATA ANALYTICS')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPJ-s7_-gIR6"
      },
      "source": [
        "## 5.Test and evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvham1r6gNnj",
        "outputId": "71609e6c-1488-4e3e-d85e-c56e2fa93374"
      },
      "outputs": [],
      "source": [
        "# === 3.4 Evaluate Performance ===\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "predictions = model.test(testset)\n",
        "rmse = accuracy.rmse(predictions)\n",
        "mae = accuracy.mae(predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snjCtej696cC"
      },
      "outputs": [],
      "source": [
        "# === 4.1 Predict for Unknown Items ===\n",
        "print(\"--- Generating Predictions for all missing pairs ---\")\n",
        "anti_testset = trainset.build_anti_testset()\n",
        "all_predictions = model.test(anti_testset)\n",
        "\n",
        "# === 4.2 Helper Function for Top-N ===\n",
        "def get_top_n(predictions, n=5):\n",
        "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\"\"\"\n",
        "    top_n = defaultdict(list)\n",
        "    for uid, iid, true_r, est, _ in predictions:\n",
        "        top_n[uid].append((iid, est))\n",
        "\n",
        "    for uid, user_ratings in top_n.items():\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_n[uid] = user_ratings[:n]\n",
        "\n",
        "    return top_n\n",
        "\n",
        "# === 4.3 Generate Top 5 Recommendations ===\n",
        "top_n_recommendations = get_top_n(all_predictions, n=5)\n",
        "print(f\"Generated recommendations for {len(top_n_recommendations)} students.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLxgr8tLVV0k"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# === Calculate Precision@K and Recall@K ===================\n",
        "# =========================================================\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "# Build Ground Truth:\n",
        "actual_courses_all = defaultdict(list)\n",
        "actual_courses_B = defaultdict(list)\n",
        "\n",
        "for uid, iid, true_r, est, _ in predictions:\n",
        "    actual_courses_all[uid].append(iid)\n",
        "\n",
        "    if true_r >= 3.0:  # grade >= B\n",
        "        actual_courses_B[uid].append(iid)\n",
        "\n",
        "\n",
        "def precision_recall_at_k(top_n, actual_dict, K=5):\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "\n",
        "    for uid, user_recs in top_n.items():\n",
        "        recommended_items = [iid for iid, _ in user_recs[:K]]\n",
        "        actual_items = actual_dict.get(uid, [])\n",
        "\n",
        "        if len(actual_items) == 0:\n",
        "            continue\n",
        "\n",
        "        true_positives = len(set(recommended_items) & set(actual_items))\n",
        "\n",
        "        precision = true_positives / K\n",
        "        recall = true_positives / len(actual_items)\n",
        "\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "\n",
        "    precision_avg = sum(precisions) / len(precisions)\n",
        "    recall_avg = sum(recalls) / len(recalls)\n",
        "    return precision_avg, recall_avg\n",
        "\n",
        "\n",
        "# --------- Calculate results for K = 5 and 10 -----------\n",
        "K_values = [5, 10]\n",
        "\n",
        "print(\"\\n====================== Precision & Recall ======================\")\n",
        "\n",
        "for K in K_values:\n",
        "    precision_all, recall_all = precision_recall_at_k(top_n_recommendations, actual_courses_all, K)\n",
        "    precision_B, recall_B = precision_recall_at_k(top_n_recommendations, actual_courses_B, K)\n",
        "\n",
        "    print(f\"\\n========== K = {K} ==========\")\n",
        "    print(\"-- Using ALL enrolled courses --\")\n",
        "    print(f\"Precision@{K}: {precision_all:.4f}\")\n",
        "    print(f\"Recall@{K}: {recall_all:.4f}\")\n",
        "\n",
        "    print(\"\\n-- Using ONLY courses with grade >= B --\")\n",
        "    print(f\"Precision@{K}: {precision_B:.4f}\")\n",
        "    print(f\"Recall@{K}: {recall_B:.4f}\")\n",
        "\n",
        "print(\"===================================================================\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHpNvrN6-ESD"
      },
      "outputs": [],
      "source": [
        "# === 5.1 Visualization Function ===\n",
        "def visualize_topk_by_rank(top_n_recommendations, K=5, top_m=10):\n",
        "    \"\"\"\n",
        "    Visualizes the frequency of recommended courses by rank.\n",
        "    K: Number of recommendations per student.\n",
        "    top_m: Number of top courses to show in the chart.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for student_id, recs in top_n_recommendations.items():\n",
        "        for r, (course, score) in enumerate(recs[:K], start=1):\n",
        "            rows.append({\"rank\": r, \"course\": course})\n",
        "\n",
        "    if not rows:\n",
        "        print(\"No recommendations to visualize.\")\n",
        "        return\n",
        "\n",
        "    df_viz = pd.DataFrame(rows)\n",
        "    rank_counters = {r: Counter(df_viz[df_viz[\"rank\"] == r][\"course\"]) for r in range(1, K+1)}\n",
        "\n",
        "    for r in range(1, K+1):\n",
        "        counter = rank_counters[r]\n",
        "        if not counter: continue\n",
        "\n",
        "        most_common = counter.most_common(top_m)\n",
        "        courses = [c for c, _ in most_common]\n",
        "        counts  = [cnt for _, cnt in most_common]\n",
        "\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.bar(courses, counts, color='skyblue')\n",
        "        plt.title(f\"Top {top_m} Courses @ Rank {r}\")\n",
        "        plt.xlabel(\"Course\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.xticks(rotation=45, ha=\"right\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# === 5.2 Run Visualization ===\n",
        "print(\"--- Visualization of Recommendations ---\")\n",
        "visualize_topk_by_rank(top_n_recommendations, K=5, top_m=10)\n",
        "\n",
        "# === 5.3 Create Final DataFrame ===\n",
        "rows = []\n",
        "for student_id, recs in top_n_recommendations.items():\n",
        "    for rank, (course, predicted_grade) in enumerate(recs, start=1):\n",
        "        # Note: Data was filtered for INT in step 2, so this check is just a safeguard\n",
        "        if isinstance(course, str) and course.startswith(\"INT\"):\n",
        "            rows.append({\n",
        "                \"student_id\": student_id,\n",
        "                \"rank\": rank,\n",
        "                \"course\": course,\n",
        "                \"predicted_grade\": predicted_grade\n",
        "            })\n",
        "\n",
        "df_recommendations = pd.DataFrame(rows)\n",
        "\n",
        "print(\"\\n--- Final Recommendations Preview ---\")\n",
        "display(df_recommendations.head(15))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ-UK6fV9mgy"
      },
      "source": [
        "## 6.Confusion Matrix 8 grades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-yjtQ4yN9pLN",
        "outputId": "460d0504-4398-4d23-807a-2e9a0042ecfb"
      },
      "outputs": [],
      "source": [
        "# 1. Define Mapping: Score -> Letter\n",
        "# ต้องเรียงลำดับจากน้อยไปมาก เพื่อให้ Matrix สวยงาม\n",
        "score_to_letter = {\n",
        "    0.0: 'F', 1.0: 'D', 1.5: 'D+', 2.0: 'C', 2.5: 'C+',\n",
        "    3.0: 'B', 3.5: 'B+', 4.0: 'A'\n",
        "}\n",
        "\n",
        "# สร้าง list ของเกรดที่เป็นไปได้ (เอาไว้ระบุ Labels)\n",
        "# กรองเอาเฉพาะที่มีใน map (เผื่อกรณีข้อมูลไม่มี F)\n",
        "valid_scores = sorted(score_to_letter.keys())\n",
        "valid_labels = [score_to_letter[s] for s in valid_scores]\n",
        "\n",
        "# 2. Helper function: หาเกรดที่ใกล้ที่สุด\n",
        "def get_nearest_grade_key(pred_score):\n",
        "    # หา key (คะแนน) ที่ใกล้ที่สุด\n",
        "    return min(valid_scores, key=lambda x: abs(x - pred_score))\n",
        "\n",
        "# 3. Prepare Data\n",
        "y_true_letters = []\n",
        "y_pred_letters = []\n",
        "\n",
        "for pred in predictions:\n",
        "    # 3.1 แปลง Actual Grade (r_ui) เป็น Letter\n",
        "    # ใช้ get_nearest_grade_key เผื่อค่า r_ui มีทศนิยมเพี้ยนเล็กน้อย\n",
        "    true_score_key = get_nearest_grade_key(pred.r_ui)\n",
        "    y_true_letters.append(score_to_letter[true_score_key])\n",
        "\n",
        "    # 3.2 แปลง Predicted Grade (est) เป็น Letter\n",
        "    pred_score_key = get_nearest_grade_key(pred.est)\n",
        "    y_pred_letters.append(score_to_letter[pred_score_key])\n",
        "\n",
        "# 4. Generate Confusion Matrix\n",
        "# สำคัญ: ต้องใส่ labels=valid_labels เพื่อบังคับลำดับ (D -> A)\n",
        "cm = confusion_matrix(y_true_letters, y_pred_letters, labels=valid_labels)\n",
        "\n",
        "# 5. Plot Heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=valid_labels,\n",
        "            yticklabels=valid_labels)\n",
        "\n",
        "plt.title('Confusion Matrix: Actual vs Predicted Grades')\n",
        "plt.xlabel('Predicted Grade')\n",
        "plt.ylabel('Actual Grade')\n",
        "plt.show()\n",
        "\n",
        "# 6. Classification Report\n",
        "print(\"\\n--- Detailed Classification Report ---\")\n",
        "print(classification_report(y_true_letters, y_pred_letters, target_names=valid_labels, zero_division=0,labels=valid_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.1 Confusion Matrix 5 grades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mapping: 5 Grades (Collapsing Plus grades)\n",
        "score_to_letter_5 = {\n",
        "    0.0: 'F', \n",
        "    1.0: 'D', 1.5: 'D', \n",
        "    2.0: 'C', 2.5: 'C',\n",
        "    3.0: 'B', 3.5: 'B', \n",
        "    4.0: 'A'\n",
        "}\n",
        "\n",
        "valid_labels_5 = ['F', 'D', 'C', 'B', 'A']\n",
        "# We still check against the full range of possible scores\n",
        "valid_scores_all = [0.0, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]\n",
        "\n",
        "y_true_5 = [score_to_letter_5[get_nearest_grade_key(p.r_ui)] for p in predictions]\n",
        "y_pred_5 = [score_to_letter_5[get_nearest_grade_key(p.est)] for p in predictions]\n",
        "\n",
        "# Plot Heatmap 5 Grades\n",
        "plt.figure(figsize=(10, 8))\n",
        "cm_5 = confusion_matrix(y_true_5, y_pred_5, labels=valid_labels_5)\n",
        "sns.heatmap(cm_5, annot=True, fmt='d', cmap='Greens', xticklabels=valid_labels_5, yticklabels=valid_labels_5)\n",
        "plt.title('Confusion Matrix: 5 Grades')\n",
        "plt.xlabel('Predicted Grade')\n",
        "plt.ylabel('Actual Grade')\n",
        "\n",
        "print(\"\\n--- 5-Grade Classification Report ---\")\n",
        "print(classification_report(y_true_5, y_pred_5, target_names=valid_labels_5, labels=valid_labels_5, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NK-V5Q__-GT",
        "outputId": "87f09fc3-4a15-4392-e4b6-a47d5359da76"
      },
      "outputs": [],
      "source": [
        "y_true_raw = [pred.r_ui for pred in predictions]\n",
        "y_pred_raw = [pred.est for pred in predictions]\n",
        "\n",
        "y_pred_rounded = [np.round(pred.est * 2) / 2 for pred in predictions]\n",
        "\n",
        "# 2. คำนวณ R2 Score\n",
        "r2 = r2_score(y_true_raw, y_pred_raw)\n",
        "r2Rounded = r2_score(y_true_raw, y_pred_rounded)\n",
        "\n",
        "\n",
        "print(f\"R2 Score: {r2:.4f}\")\n",
        "print(f\"R2 Score Rounded: {r2Rounded:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "surprise-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
