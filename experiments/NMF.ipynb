{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa4c32d2",
   "metadata": {},
   "source": [
    "## 1.Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d140ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from surprise import NMF\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Define path\n",
    "data_path = '../datasets/student_grade.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a8b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path, low_memory=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d62216b",
   "metadata": {},
   "source": [
    "## 2.Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5df33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2.1 Load Data ===\n",
    "df = pd.read_csv(data_path, low_memory=False)\n",
    "\n",
    "# === 2.2 Transform Data (Wide to Long) ===\n",
    "id_vars = ['student_id']\n",
    "df_long = pd.melt(df, id_vars=id_vars, var_name='course', value_name='grade')\n",
    "\n",
    "# === 2.3 Clean Data ===\n",
    "# Convert grade to numeric and remove invalid/empty grades\n",
    "df_long['grade'] = pd.to_numeric(df_long['grade'], errors='coerce')\n",
    "df_long_cleaned = df_long[(df_long['grade'] > 0.0) & (df_long['grade'].notna())].copy()\n",
    "\n",
    "# === 2.4 Filter for 'INT' Courses Only ===\n",
    "# This ensures the model only learns from INT courses\n",
    "df_long_filtered = df_long_cleaned[df_long_cleaned['course'].astype(str).str.startswith('INT')].copy()\n",
    "\n",
    "print(f\"--- Data Preparation Complete ---\")\n",
    "print(f\"Total records after cleaning: {len(df_long_cleaned)}\")\n",
    "print(f\"Filtered to INT courses only: {len(df_long_filtered)}\")\n",
    "display(df_long_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90bcb19",
   "metadata": {},
   "source": [
    "## 3.Split Data to train and test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a104c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3.1 Load Data into Surprise Dataset ===\n",
    "# Define rating scale (assuming grades are 1.0 to 4.0)\n",
    "reader = Reader(rating_scale=(1, 4))\n",
    "data = Dataset.load_from_df(df_long_filtered[['student_id', 'course', 'grade']], reader)\n",
    "\n",
    "\n",
    "# # === 3.2 Split Data ===\n",
    "trainset, testset = train_test_split(data, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b5d53",
   "metadata": {},
   "source": [
    "## 4.Model Training (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Training SVD Model ---\")\n",
    "model = NMF(\n",
    "    n_factors = 130,\n",
    "    n_epochs  = 90,\n",
    "    reg_pu    = 0.04,\n",
    "    reg_qi   = 0.08,\n",
    "    random_state = 42\n",
    ")\n",
    "# model = NMF(\n",
    "#     n_factors = 130,\n",
    "#     n_epochs  = 110,\n",
    "#     reg_pu    = 0.04,\n",
    "#     reg_qi   = 0.04,\n",
    "#     random_state = 42\n",
    "# )\n",
    "model.fit(trainset)\n",
    "print(\"Training complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95589c3",
   "metadata": {},
   "source": [
    "## 5.Test and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c83d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3.4 Evaluate Performance ===\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "predictions = model.test(testset)\n",
    "rmse = accuracy.rmse(predictions)\n",
    "mae = accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3550bd",
   "metadata": {},
   "source": [
    "## 6.Confusion Matrix Classification_report 8 grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553493de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping: 8 Grades\n",
    "score_to_letter_8 = {\n",
    "    0.0: 'F', 1.0: 'D', 1.5: 'D+', 2.0: 'C', 2.5: 'C+',\n",
    "    3.0: 'B', 3.5: 'B+', 4.0: 'A'\n",
    "}\n",
    "valid_scores_8 = sorted(score_to_letter_8.keys())\n",
    "valid_labels_8 = [score_to_letter_8[s] for s in valid_scores_8]\n",
    "\n",
    "def get_nearest_grade_8(score):\n",
    "    return min(valid_scores_8, key=lambda x: abs(x - score))\n",
    "\n",
    "y_true_8 = [score_to_letter_8[get_nearest_grade_8(p.r_ui)] for p in predictions]\n",
    "y_pred_8 = [score_to_letter_8[get_nearest_grade_8(p.est)] for p in predictions]\n",
    "\n",
    "# Plot Heatmap 8 Grades\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm_8 = confusion_matrix(y_true_8, y_pred_8, labels=valid_labels_8)\n",
    "sns.heatmap(cm_8, annot=True, fmt='d', cmap='Blues', xticklabels=valid_labels_8, yticklabels=valid_labels_8)\n",
    "plt.title('Confusion Matrix: 8 Grades')\n",
    "plt.xlabel('Predicted Grade')\n",
    "plt.ylabel('Actual Grade')\n",
    "\n",
    "print(\"--- 8-Grade Classification Report ---\")\n",
    "print(classification_report(y_true_8, y_pred_8, target_names=valid_labels_8, labels=valid_labels_8, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60eaa72",
   "metadata": {},
   "source": [
    "## 6.1 Confusion Matrix Classification_report 5 grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea1ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping: 5 Grades (Collapsing Plus grades)\n",
    "score_to_letter_5 = {\n",
    "    0.0: 'F', \n",
    "    1.0: 'D', 1.5: 'D', \n",
    "    2.0: 'C', 2.5: 'C',\n",
    "    3.0: 'B', 3.5: 'B', \n",
    "    4.0: 'A'\n",
    "}\n",
    "valid_labels_5 = ['F', 'D', 'C', 'B', 'A']\n",
    "# We still check against the full range of possible scores\n",
    "valid_scores_all = [0.0, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]\n",
    "\n",
    "y_true_5 = [score_to_letter_5[get_nearest_grade_8(p.r_ui)] for p in predictions]\n",
    "y_pred_5 = [score_to_letter_5[get_nearest_grade_8(p.est)] for p in predictions]\n",
    "\n",
    "# Plot Heatmap 5 Grades\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm_5 = confusion_matrix(y_true_5, y_pred_5, labels=valid_labels_5)\n",
    "sns.heatmap(cm_5, annot=True, fmt='d', cmap='Greens', xticklabels=valid_labels_5, yticklabels=valid_labels_5)\n",
    "plt.title('Confusion Matrix: 5 Grades')\n",
    "plt.xlabel('Predicted Grade')\n",
    "plt.ylabel('Actual Grade')\n",
    "\n",
    "print(\"\\n--- 5-Grade Classification Report ---\")\n",
    "print(classification_report(y_true_5, y_pred_5, target_names=valid_labels_5, labels=valid_labels_5, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43518cd",
   "metadata": {},
   "source": [
    "## 6.2 Confusion Matrix Classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79af0086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3.4 Evaluate Performance ===\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# ----- Regression metrics -----\n",
    "rmse = accuracy.rmse(predictions, verbose=False)\n",
    "mae  = accuracy.mae(predictions, verbose=False)\n",
    "\n",
    "# ดึงค่าจริง/ค่าทำนายออกมา\n",
    "y_true = [p.r_ui for p in predictions]   # actual grade\n",
    "y_pred = [p.est  for p in predictions]   # predicted grade\n",
    "\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"RMSE : {rmse:.4f}\")\n",
    "print(f\"MAE  : {mae:.4f}\")\n",
    "print(f\"R²   : {r2:.4f}\")\n",
    "\n",
    "# ----- Classification-style metrics -----\n",
    "# แปลงเป็น high / low ด้วย threshold เช่น >= 2.5 = class 1\n",
    "threshold = 3.0  # หรือ 3.0 ถ้าอยากโหดขึ้น\n",
    "\n",
    "y_true_cls = [1 if y >= threshold else 0 for y in y_true]\n",
    "y_pred_cls = [1 if y >= threshold else 0 for y in y_pred]\n",
    "\n",
    "# Macro = ให้น้ำหนักทุก class เท่ากัน\n",
    "precision_macro = precision_score(y_true_cls, y_pred_cls, average='macro')\n",
    "recall_macro    = recall_score(y_true_cls, y_pred_cls, average='macro')\n",
    "f1_macro        = f1_score(y_true_cls, y_pred_cls, average='macro')\n",
    "\n",
    "# Weighted = ถ่วงตามสัดส่วนคลาส (ถ้า class imbalance)\n",
    "precision_weighted = precision_score(y_true_cls, y_pred_cls, average='weighted')\n",
    "recall_weighted    = recall_score(y_true_cls, y_pred_cls, average='weighted')\n",
    "f1_weighted        = f1_score(y_true_cls, y_pred_cls, average='weighted')\n",
    "\n",
    "print(\"\\n--- Classification Metrics ---\")\n",
    "print(f\"Precision (macro)   : {precision_macro:.4f}\")\n",
    "print(f\"Recall    (macro)   : {recall_macro:.4f}\")\n",
    "print(f\"F1-score  (macro)   : {f1_macro:.4f}\")\n",
    "\n",
    "print(f\"\\nPrecision (weighted): {precision_weighted:.4f}\")\n",
    "print(f\"Recall    (weighted): {recall_weighted:.4f}\")\n",
    "print(f\"F1-score  (weighted): {f1_weighted:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534f49d6",
   "metadata": {},
   "source": [
    "## R-score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bf43b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_raw = [pred.r_ui for pred in predictions]\n",
    "y_pred_raw = [pred.est for pred in predictions]\n",
    "\n",
    "y_pred_rounded = [np.round(pred.est * 2) / 2 for pred in predictions]\n",
    "\n",
    "# 2. คำนวณ R2 Score\n",
    "r2 = r2_score(y_true_raw, y_pred_raw)\n",
    "r2Rounded = r2_score(y_true_raw, y_pred_rounded)\n",
    "\n",
    "\n",
    "print(f\"R2 Score: {r2:.4f}\")\n",
    "print(f\"R2 Score Rounded: {r2Rounded:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surprise-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
